{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to put the Google data into a useful format\n",
    "def reformat(infile, outfile, dictfile, min_year):\n",
    "    \n",
    "    raw = pd.read_csv(infile, delimiter='\\t', names=[\"1gram\", \"year\", \"occ\", \"unused1\", \"unused2\"], \n",
    "                      usecols=[\"1gram\", \"year\", \"occ\"],\n",
    "                      converters={\"1gram\":str, \"year\":int, \"occ\":int},\n",
    "                      engine='c', quoting=3, encoding='latin-1')\n",
    "    \n",
    "    # break up each word into its own sub-array\n",
    "    break_inds = np.where(raw[\"1gram\"] != np.roll(raw[\"1gram\"], 1))[0]\n",
    "    \n",
    "    years = np.array(raw[\"year\"])\n",
    "    years_each_word = np.array(np.split(years, break_inds[1:]))\n",
    "    \n",
    "    all_words = np.array(raw[\"1gram\"])[break_inds]\n",
    "    \n",
    "    # only use data from words without \"wacky\" characters and that have appeared continuously\n",
    "    usable_inds = []\n",
    "    for index in range(years_each_word.size):\n",
    "        try:\n",
    "            if all_words[index] == all_words[index].encode(\"latin-1\").decode(\"utf-8\"):\n",
    "                #if (years_each_word[index] - np.roll(years_each_word[index], 1) < 2).all():\n",
    "                    #if years_each_word[index].min() <= 1800 and years_each_word[index].max() == 2008:\n",
    "                usable_inds.append(index)\n",
    "        except UnicodeDecodeError:\n",
    "            print(f'WARNING - unicode decode error, ignoring word {index}...')\n",
    "    \n",
    "    if outfile:\n",
    "        # writing the counts for each word\n",
    "        \n",
    "        years_each_word = years_each_word[usable_inds]\n",
    " \n",
    "        counts = np.array(raw[\"occ\"])\n",
    "        counts_each_word = np.array(np.split(counts, break_inds[1:]))\n",
    "        counts_each_word = counts_each_word[usable_inds]\n",
    "        \n",
    "        max_year = 2008\n",
    "       \n",
    "        for i in range(years_each_word.size):\n",
    "            # get rid of years pre-min_year\n",
    "            inds = np.where(years_each_word[i] >= min_year)\n",
    "            years_each_word[i] = years_each_word[i][inds]\n",
    "            counts_each_word[i] = counts_each_word[i][inds]\n",
    "    \n",
    "        # axes (word, year)\n",
    "        full_counts_each_word = np.zeros((len(usable_inds), max_year-min_year+1))\n",
    "        \n",
    "        for i in range(full_counts_each_word.shape[0]):\n",
    "            full_counts_each_word[i][years_each_word[i].astype(np.int32)-min_year] = counts_each_word[i]\n",
    "\n",
    "        # write counts\n",
    "        with open(outfile, 'ba') as f:\n",
    "            np.savetxt(f, full_counts_each_word, fmt='%u', delimiter=',')\n",
    "    \n",
    "    if dictfile:\n",
    "\n",
    "        usable_words = all_words[usable_inds]\n",
    "\n",
    "        # write words to ordered \"dictionary\"\n",
    "        with open(dictfile, 'a+') as f:\n",
    "            for word in usable_words:\n",
    "                f.write(word)\n",
    "                f.write('\\n')\n",
    "                \n",
    "# to get a count of how many of each word is used in a book\n",
    "def get_words(dictfile, bookfile):\n",
    "    \n",
    "    # read a dictionary file, an ordered list of strings\n",
    "    # ordered the same as the words are in the data\n",
    "    with open(dictfile, 'r') as f:\n",
    "        dct = f.read().splitlines()\n",
    "    \n",
    "    with open(bookfile, 'r') as file:\n",
    "        book = file.read().replace('\\n', ' ')\n",
    "\n",
    "    # we read the book and count how many times each word in our dictionary appears\n",
    "\n",
    "    raw_words = book.split()\n",
    "    word_counts = Counter(raw_words)\n",
    "\n",
    "    words = np.zeros(np.size(dct))\n",
    "    for i, word in enumerate(dct):\n",
    "        words[i] += word_counts[word]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now some stuff we need for Bayesian stats...\n",
    "\n",
    "def norm_data(data):\n",
    "    \n",
    "    # ensure multinomial coeffs for each year sum to 1\n",
    "    norm = np.sum(data, axis=0)\n",
    "    \n",
    "    print('sum ok.')\n",
    "    \n",
    "    return data / norm\n",
    "\n",
    "def likelihood(years, words, data):\n",
    "    \n",
    "    # get probability that all of words were picked given data at year, for each year\n",
    "    \n",
    "    print('norm ok.')\n",
    "    \n",
    "    # axes of data are (word, year). loop over years (to sacrifice speed for memory)\n",
    "    multinom = np.zeros_like(data[0])\n",
    "    \n",
    "    print('zeros ok.')\n",
    "    \n",
    "    for i in range(data[0].size):\n",
    "        \n",
    "        multinom[i] = np.sum(words*np.log(data[:,i]))\n",
    "    \n",
    "    print('multinom ok.')\n",
    "    \n",
    "    # get normalization factor\n",
    "    norm = np.sum(multinom*dx)\n",
    "    print(norm)\n",
    "    \n",
    "    print('overall norm ok.')\n",
    "    \n",
    "    return multinom\n",
    "    \n",
    "def exp_prior(years):\n",
    "    \n",
    "    # an arbitrary exponential prior\n",
    "    # to capture greater number of publications in recent years\n",
    "    \n",
    "    return np.log(np.exp(0.01*years)) / (np.sum(np.log(np.exp(0.01*years)))*dx)\n",
    "\n",
    "def uni_prior(years):\n",
    "    \n",
    "    return np.zeros_like(years)\n",
    "\n",
    "def posterior(years, words, data):\n",
    "    \n",
    "    # feel free to swap the priors and see what changes\n",
    "    post = likelihood(years, words, data) + uni_prior(years)\n",
    "    \n",
    "    norm = np.sum(post*dx)\n",
    "    \n",
    "    return post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by downloading the Google Ngram \"English 1 Million\" dataset (the 1grams only)\n",
    "# put them in a folder ../ngram_data relative to where this script lives\n",
    "# \n",
    "# then, run the next cell *once* (you may want to go for a walk or something)\n",
    "# if you run it again, choose different file names, as the program appends\n",
    "\n",
    "# furthest in the past you wish to delve\n",
    "min_year = 1800\n",
    "# file names\n",
    "outfile = \"../ngram_data/full_counts_nice.csv\"\n",
    "dictfile = \"../ngram_data/py_dict_nice.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done file 0/9\n",
      "done file 1/9\n",
      "done file 2/9\n",
      "done file 3/9\n",
      "done file 4/9\n",
      "done file 5/9\n",
      "done file 6/9\n",
      "done file 7/9\n",
      "WARNING - unicode decode error, ignoring word 0...\n",
      "done file 8/9\n",
      "done file 9/9\n"
     ]
    }
   ],
   "source": [
    "# we put the Google data in a usable format in these files\n",
    "for i in range(10):\n",
    "    name = f\"../ngram_data/googlebooks-eng-1M-1gram-20090715-{i}.csv\"\n",
    "    reformat(name, outfile, dictfile, min_year)\n",
    "    print(f\"done file {i}/9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, find (or write) a book and put it into a .txt file\n",
    "bookfile = '../ngram_data/test.txt'\n",
    "\n",
    "words = get_words(dictfile, bookfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we read the data again\n",
    "# but we won't lose it forever if (when) the kernel dies\n",
    "\n",
    "data = pd.read_csv(outfile, delimiter=',', dtype=np.int32, header=None).values\n",
    "# this was my fix for being able to work in logspace\n",
    "# it seems reasonable, as these are the 1 million most common words\n",
    "data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum ok.\n"
     ]
    }
   ],
   "source": [
    "# the fun step!\n",
    "# note we are working with logarithmic probability\n",
    "\n",
    "dx = 1\n",
    "years = np.arange(1800, 2008+1, dx)\n",
    "\n",
    "# generate posterior\n",
    "\n",
    "post = posterior(years, words, norm_data(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make nice plots\n",
    "\n",
    "plt.plot(years, post)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
